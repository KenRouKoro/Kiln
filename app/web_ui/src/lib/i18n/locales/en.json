{
  "common": {
    "save": "Save",
    "cancel": "Cancel",
    "delete": "Delete",
    "edit": "Edit",
    "create": "Create",
    "update": "Update",
    "close": "Close",
    "continue": "Continue",
    "retry": "Retry",
    "loading": "Loading...",
    "error": "Error",
    "success": "Success",
    "warning": "Warning",
    "info": "Info",
    "yes": "Yes",
    "no": "No",
    "ok": "OK",
    "back": "Back",
    "next": "Next",
    "previous": "Previous",
    "submit": "Submit",
    "reset": "Reset",
    "clear": "Clear",
    "search": "Search",
    "filter": "Filter",
    "sort": "Sort",
    "export": "Export",
    "import": "Import",
    "download": "Download",
    "upload": "Upload",
    "view": "View",
    "add": "Add",
    "remove": "Remove",
    "copy": "Copy",
    "paste": "Paste",
    "cut": "Cut",
    "select": "Select",
    "all": "All",
    "none": "None",
    "refresh": "Refresh",
    "reload": "Reload",
    "back_home": "Return Home",
    "change": "Change",
    "accept": "Accept",
    "reject": "Reject",
    "show": "Show",
    "hide": "Hide",
    "version": "Version",
    "tags": "Tags",
    "or": "Or",
    "has": "has",
    "samples": "samples",
    "before_you_start": "before you start",
    "id": "ID",
    "recommended": "Recommended",
    "and": " and ",
    "task": "Task"
  },
  "navigation": {
    "run": "Run",
    "dataset": "Dataset",
    "synthetic_data": "Synthetic Data",
    "fine_tune": "Fine Tune",
    "evals": "Evals",
    "prompts": "Prompts",
    "settings": "Settings"
  },
  "project": {
    "name": "Project Name",
    "description": "Project Description",
    "create_project": "Create Project",
    "add_project": "Add Project",
    "add_project_subtitle": "Projects are a collection of tasks, results, evals, and other resources.",
    "edit_project": "Edit Project",
    "manage_projects": "Manage Projects",
    "manage_projects_subtitle": "Add or remove projects",
    "project_created": "Project Created!",
    "project_imported": "Project Imported!",
    "import_project": "Import Project",
    "existing_project_path": "Existing Project Path",
    "import_existing_project": "import an existing project",
    "create_new_project": "create a new project",
    "update_project": "Update Project",
    "project_created_message": "Your new project \"{name}\" has been created.",
    "project_imported_message": "Your project \"{path}\" has been imported.",
    "project_path_description": "The path to the project on your local machine. For example, /Users/username/Kiln Projects/my_project/project.kiln",
    "add_task": "Add Task",
    "current": "Current",
    "no_projects_found": "No projects found",
    "remove_project_confirm": "Are you sure you want to remove the project \"{name}\"?\n\nThis will remove it from the UI, but won't delete files from your disk.",
    "failed_to_remove_project": "Failed to remove project.\n\nReason: {error}",
    "create_project_subtitle": "\"Example\" is fine if you're just trying things out.",
    "just_exploring": "Just exploring?",
    "create_example": "Create an example",
    "example_project_name": "Example Project",
    "example_project_description": "This is an example project just to try things out."
  },
  "task": {
    "name": "Task Name",
    "description": "Task Description",
    "instructions": "Task Instructions",
    "create_task": "Create Task",
    "new_task": "New Task",
    "new_task_description": "A 'task' is a single goal for a model to accomplish.",
    "target_project": "Target project",
    "edit_task": "Edit Task",
    "clone_task": "Clone Task",
    "clone_task_subtitle": "Create a new task, using an existing task as a template",
    "clone_task_description": "The cloned task will not contain any data from the original task.",
    "current_task": "Current Task",
    "select_task": "Select Task",
    "task_created": "Task Created!",
    "prompt_description": "The prompt for the model to follow.",
    "task_id": "Task ID",
    "error_loading_task": "Error loading task",
    "task_access_error": "Could not load task. It may belong to a project you don't have access to.",
    "project_or_task_id_missing": "Project or task ID not set.",
    "copy_of": "Copy of {name}",
    "schema_section": {
      "plain_text": "Plain Text",
      "structured_json": "Structured JSON"
    },
    "create_task_page": {
      "title": "Create a Task",
      "subtitle": "Let's define what this model should do. We call this a \"task\"."
    },
    "edit_task_page": {
      "part_1_overview": "Part 1: Overview",
      "just_exploring": "Just exploring?",
      "try_example": "Try an example.",
      "task_name_label": "Task Name",
      "task_name_description": "A description for you and your team, not used by the model.",
      "prompt_task_instructions_label": "Prompt / Task Instructions",
      "task_description_label": "Task Description",
      "task_description_description": "A description for you and your team, not used by the model.",
      "thinking_instructions_label": "'Thinking' Instructions",
      "thinking_instructions_description": "Instructions for how the model should 'think' about the task prior to answering. Used for chain of thought style prompting.",
      "thinking_instructions_info": "Used when running a 'Chain of Thought' prompt. If left blank, a default 'think step by step' prompt will be used. Optionally customize this with your own instructions to better fit this task.",
      "part_2_requirements": "Part 2: Requirements",
      "requirements_description": "Define requirements you can use to rate the results of the model. These are used in the prompt, ratings, evals and training.",
      "learn_more": "Learn more",
      "requirement_name_label": "Requirement Name",
      "requirement_name_info": "A short name to uniquely identify the requirement. This will appear in the rating UI. It's not used by the model.",
      "rating_type_label": "Rating Type",
      "priority_label": "Priority",
      "requirement_instructions_label": "Instructions: a few sentences describing this requirement for the model",
      "requirement_instructions_info": "These instructions will be appended to the prompt and using during evals. It should be written with the model in mind. Example requirement: Name='Be Succinct' and Instruction='Use short sentences and simple language. Don't repeat yourself.'",
      "input_schema_title": "Input Schema",
      "output_schema_title": "Output Schema",
      "input_schema_description": "What kind of input will the model receive?",
      "output_schema_description": "What kind of output will the model produce?",
      "cannot_edit_existing_schema": "You can't edit an existing task's {type} format, as existing dataset items would not conform to the new schema.",
      "clone_task_instead": "You can",
      "clone_this_task": "clone this task",
      "instead": "instead.",
      "save_task": "Save Task",
      "create_task": "Create Task",
      "replace_edits_confirm": "This will replace your current task edits. Are you sure?",
      "example_task_name": "Joke Generator",
      "example_task_description": "An example task from the KilnAI team.",
      "example_task_instruction": "Generate a joke, given a theme. The theme will be provided as a word or phrase as the input to the model. The assistant should output a joke that is funny and relevant to the theme. If a style is provided, the joke should be in that style. The output should include a setup and punchline.",
      "prompt_description_editing_with_requirements": "The base prompt used by prompt generators (Basic, Multi-shot, etc). The task requirements below are appended to this. You can create additional prompts in the 'Prompts' tab to compare prompt performance.",
      "prompt_description_editing_no_requirements": "The base prompt used by prompt generators (Basic, Multi-shot, etc). You can create additional prompts in the 'Prompts' tab to compare prompt performance.",
      "prompt_description_creating": "The prompt for the model to follow.",
      "rating_types": {
        "five_star": "5 Star",
        "pass_fail": "Pass / Fail",
        "pass_fail_critical": "Pass / Fail / Critical"
      },
      "priorities": {
        "p0_critical": "P0 - Critical",
        "p1_high": "P1 - High",
        "p2_medium": "P2 - Medium",
        "p3_low": "P3 - Low"
      },
      "input_format_plain_text": "Input Format: Plain text",
      "output_format_plain_text": "Output Format: Plain text",
      "part": "Part",
      "project_required_error": "You must create a project before creating a task",
      "current_project_not_found": "Current project not found",
      "requirement_label": "Requirement",
      "example_joke_topic_title": "Joke Topic",
      "example_joke_topic_description": "The topic of the joke.",
      "example_joke_style_title": "Joke Style",
      "example_joke_style_description": "The style of the joke, such as 'dad joke' or 'kids joke'.",
      "example_setup_title": "setup",
      "example_setup_description": "The setup to the joke",
      "example_punchline_title": "punchline",
      "example_punchline_description": "The punchline to the joke",
      "project_id_prefix": "Project ID: "
    }
  },
  "settings": {
    "title": "Settings",
    "edit_task": "Edit Task",
    "edit_task_description": "Edit the currently task, including the prompt and requirements.",
    "edit_current_task": "Edit Current Task",
    "ai_providers": "AI Providers & Models",
    "ai_providers_description": "Connect to AI providers like OpenAI, OpenRouter, or Ollama.",
    "read_the_docs": "Read the Docs",
    "manage_providers": "Manage Providers & Models",
    "manage_projects_description": "Add, remove or edit projects.",
    "edit_project_description": "Edit the currently selected project.",
    "edit_current_project": "Edit Current Project",
    "app_updates": "App Updates",
    "app_updates_description": "Check if there is a new version of the app available.",
    "check_for_update": "Check for Update",
    "replay_introduction": "Replay Introduction",
    "replay_intro_description": "Watch the introduction slide-show.",
    "play_intro": "Play Intro",
    "license": "License",
    "license_description": "View the Kiln AI desktop app License Agreement.",
    "view_eula": "View EULA",
    "language": "Language",
    "language_description": "Change the application language."
  },
  "dataset": {
    "title": "Dataset",
    "read_the_docs": "Read the Docs",
    "add_data": "Add Data",
    "view_dataset": "View Dataset",
    "data_added": "Data Added",
    "data_added_subtitle": "Your data has been added.",
    "manually_tag_existing_data": "Manually Tag Existing Data",
    "open_dataset": "Open Dataset",
    "error_loading": "Error Loading Dataset",
    "filter_by_tags": "Filter Dataset by Tags",
    "current_filters": "Current Filters:",
    "add_filter": "Add a filter:",
    "no_more_filters": "Any further filters would show zero results.",
    "delete_runs": "Delete Runs",
    "delete_run": "Delete Run",
    "cannot_undo": "This cannot be undone.",
    "add_tags_to_runs": "Add Tags to Runs",
    "add_tags_to_run": "Add Tags to Run",
    "tags_organize": "Tags can be used to organize you dataset.",
    "remove_tags_from_runs": "Remove Tags from Runs",
    "remove_tags_from_run": "Remove Tags from Run",
    "selected_tags_remove": "Selected tags to remove:",
    "no_tags_selected": "No tags selected.",
    "available_tags": "Available tags:",
    "no_tags_on_runs": "No tags on selected runs.",
    "all_tags_selected": "All available tags already selected.",
    "rating": "Rating",
    "repair_state": "Repair State",
    "source": "Source",
    "model": "Model",
    "created_at": "Created At",
    "input_preview": "Input Preview",
    "output_preview": "Output Preview",
    "no_input": "No input",
    "no_output": "No output",
    "empty_dataset_title": "Your dataset for this task is empty.",
    "empty_dataset_description": "Adding data will allow the model to improve at it's task.",
    "empty_dataset_instruction": "To get started, generate some synthetic data or add data manually",
    "manually_add_data": "Manually Add Data",
    "generate_synthetic_data": "Generate Synthetic Data",
    "run_title": "Dataset Run",
    "run_id": "Run ID",
    "run_deleted": "Run Deleted",
    "run_not_found": "Run not found",
    "input": "Input",
    "parameters": "Parameters",
    "input_source": "Input Source",
    "output_model": "Output Model",
    "model_provider": "Model Provider",
    "prompt": "Prompt",
    "cost": "Cost",
    "tokens": "Tokens",
    "created_by": "Created By",
    "topic": "Topic",
    "could_not_load_run": "Could not load run. It may belong to a project you don't have access to.",
    "upload_dialog": {
      "title": "Upload CSV to Dataset",
      "description": "Upload a CSV to add each row to your dataset. The CSV must have a header row",
      "see_docs": "see docs",
      "supported_columns": "The following columns are supported:",
      "input_required": "Required",
      "output_required": "Required",
      "reasoning_optional": "Optional",
      "chain_of_thought_optional": "Optional",
      "tags_optional": "Optional, comma separated string"
    }
  },
  "run": {
    "title": "Run",
    "input": "Input",
    "options": "Options",
    "clear_all": "Clear All",
    "next_run": "Next Run",
    "task_prefix": "Task: ",
    "required": "Required",
    "model_selection_error": "You must select a model before running",
    "choose_prompt_description": "Choose a prompt. Learn more on the 'Prompts' tab.",
    "output_rating": "Output Rating",
    "overall_rating": "Overall Rating:",
    "edit": "Edit",
    "retry_repair": "Retry Repair",
    "accept_repair": "Accept Repair (5 Stars)",
    "error_accepting_repair": "Error Accepting Repair",
    "manual_repair": "Manual Repair",
    "manual_repair_description": "Provide a improvement or correction to the task's output",
    "save_repair": "Save Repair (5 stars)",
    "error_saving_repair": "Error Saving Repair",
    "prompt_method": "Prompt Method",
    "prompt_generators": "Prompt Generators",
    "fine_tune_prompt": "Fine-Tune Prompt",
    "fine_tune_specific_prompt": "Fine-Tune Specific Prompt",
    "fine_tune_prompt_description": "Recommended: The prompt used to fine-tune this model.",
    "custom_prompt": "Custom Prompt",
    "saved_prompts": "Saved Prompts",
    "fine_tune_model_warning": "We strongly recommend using prompt the model was trained on when running a fine-tuned model.",
    "plaintext_input": "Plaintext Input",
    "invalid_unsupported_schema": "Invalid or unsupported input schema",
    "output": "Output",
    "structure_valid": "Structure Valid",
    "show_raw_data": "Show Raw Data",
    "hide_raw_data": "Hide Raw Data",
    "raw_data": "Raw Data",
    "repair_output": "Repair Output",
    "repair_not_available": "Repair is not available for runs from {source} sources.",
    "repair_instructions": "Repair Instructions",
    "repair_instructions_description": "Since the output isn't 5-star, provide instructions for the model on how to fix it.",
    "attempt_repair": "Attempt Repair",
    "manual_edit_description": "Manually improve or correct the response.",
    "repair_review_description": "The model has attempted to fix the output given your instructions. Review the result.",
    "your_instructions": "your instructions",
    "no_instruction_provided": "No instruction provided",
    "repair_complete_user": "This repaired output was provided by {name}.",
    "repair_complete_model": "The model has fixed the output given your instructions.",
    "delete_repair": "Delete Repair",
    "error_deleting_repair": "Error Deleting Repair:",
    "delete_repair_confirm": "Are you sure you want to delete this repair?\n\nThis action cannot be undone.",
    "intermediate_outputs": {
      "reasoning": "Model Reasoning Output",
      "chain_of_thought": "Chain of Thought Output"
    },
    "intermediate_output_tooltip": "This is intermediate output from the model, and not considered part of the final answer. This thinking helped formulate the final answer above. This is known as 'chain of thought', 'thinking output', or 'inference time compute'.",
    "type_descriptions": {
      "string": "String",
      "number": "Number",
      "integer": "Integer",
      "boolean": "'true' or 'false'",
      "array": "JSON Array",
      "object": "JSON Object",
      "unknown": "Unknown type",
      "required_suffix": " (required)",
      "optional_suffix": " (optional)"
    },
    "input_info": {
      "array_description": "A list of items in JSON format. For example: [item_1, item_2]",
      "object_description": "A JSON object. For example: {\"key_1\": \"value_1\", \"key_2\": \"value_2\"}"
    }
  },
  "generate": {
    "title": "Generate",
    "no_items_to_save": "No Items to Save",
    "generate_data_to_start": "Generate some data to get started.",
    "existing_items_saved": "existing items already saved.",
    "samples_failed_to_save": "samples failed to save. Running again may resolve transient issues.",
    "show_errors": "Show Errors",
    "hide_errors": "Hide Errors",
    "error_message": "Error message:",
    "synthetic_data_generation": "Synthetic Data Generation",
    "read_the_docs": "Read the Docs",
    "save_all": "Save All",
    "add_guidance": "Add Guidance",
    "edit_guidance": "Edit Guidance",
    "save_all_items": "Save All Items",
    "run_generation": "Run generation and add all items to your dataset.",
    "status": "Status",
    "items_pending": "items pending",
    "already_saved": "already saved",
    "human_guidance_enabled": "Human guidance is enabled, but you've selected a custom prompt with a fixed string. Human guidance will not be applied.",
    "human_guidance_warning": "Human guidance is enabled. Your guidance will be passed to the model and used to influence output.",
    "run_mode": "Run Mode",
    "parallel_mode": "Parallel - Ideal for APIs (OpenAI, Fireworks)",
    "sequential_mode": "Sequential - Ideal for Ollama",
    "run_mode_description": "Parallel is ideal for APIs (OpenAI, Fireworks, etc.) as they can handle thousands of requests in parallel. Sequential is ideal for Ollama or other servers that can only handle one request at a time.",
    "run_and_save": "Run and Save",
    "saved_new_items": "Saved {count} new items.",
    "use_dataset_tab": "Use the",
    "dataset_tab": "dataset tab",
    "to_review_manage": "to review and manage.",
    "set_tagged_with": "Set tagged with \"{tag}\"",
    "complete": "complete",
    "failed": "failed",
    "content_generation_running": "Content generation is currently running. If you leave, it will be stopped and your changes will be lost.\n\nPress Cancel to stay, OK to leave.",
    "unsaved_changes_warning": "You have unsaved changes which will be lost if you leave.\n\nPress Cancel to stay, OK to leave.",
    "topic_tree_warning": "Your topic tree will be lost if you leave.\n\nPress Cancel to stay, OK to leave.",
    "close": "close",
    "failed_to_save_sample": "Failed to save sample",
    "no_id_returned": "No ID returned from server",
    "samples_modal": {
      "title": "Generate Data",
      "subtitle": "Add synthetic data samples",
      "subtitle_to": "to",
      "subtitle_each_subtopic": "each subtopic of",
      "sample_count": "Sample Count",
      "failed_to_generate": "Failed to generate samples for {topic}. Running again may resolve transient issues.",
      "topics_failed": "{count} topics failed. Running again may resolve transient issues.",
      "generate_samples": "Generate {count} Samples",
      "for_each_topic": "For Each Topic",
      "no_model_selected": "No model selected.",
      "invalid_model_selected": "Invalid model selected.",
      "no_options_returned": "No options returned.",
      "could_not_generate_topics": "Could not generate topics, unknown error. If it persists, try another model."
    }
  },
  "evaluation": {
    "title": "Evaluations",
    "page_title": "Evals",
    "page_subtitle": "Evaluate task performance. Compare models, prompts, and fine-tunes.",
    "read_docs": "Read the Docs",
    "new_evaluator": "New Evaluator",
    "eval_name": "Eval Name",
    "description": "Description",
    "selected_run_method": "Selected Run Method",
    "model": "Model",
    "prompt": "Prompt",
    "na": "N/A",
    "unknown_error_occurred": "An unknown error occurred",
    "create_eval": "Create Evaluation",
    "run_eval": "Run Eval",
    "eval_complete": "Eval Complete",
    "eval_complete_with_errors": "Eval Complete with Errors",
    "running": "Running...",
    "no_data_needed": "No Data Needed to be Evaluated",
    "compare_eval_methods": "Compare Evaluation Methods",
    "find_best_evaluator": "Find the evaluation method that best matches human-ratings",
    "add_eval_method": "Add Eval Method",
    "instructions": "Instructions",
    "select_all_options_error": "Select all options needed to run the eval",
    "re_run_eval": "Re-run Eval",
    "eval_status": "Eval Status",
    "add_more_data_instruction": "If you want to add more data to your eval,",
    "for_instructions": "for instructions",
    "of": "of",
    "error": "error",
    "errors_plural": "s",
    "create_evaluator": {
      "title": "Create a New Evaluator",
      "subtitle": "Evaluators judge task performance and help you find the best method of running your task.",
      "error_loading_task": "Error Loading Task Information",
      "unknown_error": "An unknown error occurred",
      "part1_title": "Part 1: Evaluator Details",
      "part2_title": "Part 2: Evaluator Output Scores",
      "part3_title": "Part 3: Task Evaluation Dataset",
      "part4_title": "Part 4: Dataset to Compare Evaluation Methods",
      "evaluator_name": "Evaluator Name",
      "evaluator_name_description": "Give your evaluator a name that will help you identify it later.",
      "evaluator_description": "Evaluator Description",
      "evaluator_description_description": "Give your evaluator a description.",
      "output_scores_description": "Define the scores that the evaluator will output.",
      "template_warning": "Since you selected a template, you can't edit these. Use the 'Custom' template to create your own scores.",
      "score_name": "Score Name",
      "rating_type": "Rating Type",
      "five_star": "5 Star",
      "pass_fail": "Pass / Fail",
      "pass_fail_critical": "Pass / Fail / Critical",
      "instructions": "Instructions",
      "evaluation_dataset": "Evaluation Dataset",
      "evaluation_dataset_description": "Specify which which part of your dataset is used when evaluating various methods of running your task. You can create this data later.",
      "evaluation_dataset_info": "You can populate this dataset later by adding this tag to samples in your dataset.",
      "filter_recommended": "Filter dataset to the '{tag}' tag (recommended)",
      "filter_custom": "Filter dataset by a custom tag",
      "use_all_data": "Use every dataset item in the evaluation (not recommended)",
      "custom_tag_label": "Evaluation Dataset Filter Tag",
      "custom_tag_description": "Your dataset will be filtered to only include items with this tag.",
      "eval_method_dataset": "Evaluation Method Dataset",
      "eval_method_dataset_description": "Specify which which part of your dataset is used when trying to find the best evaluation method for this task. You can create and rate this data later.",
      "eval_method_dataset_info": "You can populate this dataset later. We recommend you have a person rate all of the samples in this dataset, so you can compare evaluation methods to human ratings.",
      "config_tag_label": "Evaluation Config Dataset Filter Tag",
      "config_tag_description": "Your dataset will be filtered to only include items with this tag.",
      "create_evaluator": "Create Evaluator",
      "select_datasets_error": "Please select both evaluation and config datasets",
      "output_score": "Output Score",
      "select_eval_template": {
        "title": "Select Evaluator Template",
        "overall_task_performance": "Overall Task Performance",
        "overall_task_performance_description": "Evaluate overall task performance via the overall score and custom task goals.",
        "custom_goal_and_scores": "Custom Goal and Scores",
        "create_your_own": "Create Your Own",
        "custom_goal_description": "Write an evaluator from scratch. You'll be able to specify scores and write custom instructions.",
        "toxicity_evaluator": "Toxicity Evaluator",
        "toxicity_description": "Evaluate the toxicity of the model's output.",
        "bias_evaluator": "Bias Evaluator",
        "bias_description": "Evaluate the model's output for gender bias, racial bias, and other bias.",
        "maliciousness_evaluator": "Maliciousness Evaluator",
        "maliciousness_description": "Evaluate the model's output for maliciousness including deception, exploitation, and harm.",
        "factual_correctness_evaluator": "Factual Correctness Evaluator",
        "factual_correctness_description": "Evaluate the model's output for factual correctness and critical omissions.",
        "jailbreak_evaluator": "Jailbreak Evaluator",
        "jailbreak_description": "Evaluate the user's ability to break out of the prompt, using tactics such as 'ignore previous instructions'.",
        "overall_performance_eval": "Overall Performance Eval",
        "eval_goals_description": "This eval will evaluate the following goals:",
        "overall_rating": "Overall Rating",
        "edit_requirements": "Edit Requirements",
        "create_eval": "Create Eval",
        "edit_requirements_note": "To add or remove goals, 'Edit Requirements' before creating your eval.",
        "task_required_error": "Task is required for this template, and the task failed to load."
      }
    },
    "goals": {
      "define": "Define Goals",
      "create_data": "Create Eval Data",
      "human_ratings": "Human Ratings",
      "find_best_evaluator": "Find the Best Evaluator",
      "find_best_way": "Find the Best Way to Run this Task"
    },
    "improve_quality": "Improve Quality and Move Faster with Evaluations",
    "create_powerful_evaluators": "Create powerful evaluators using LLMs to judge performance.",
    "compare_approaches": "Quickly compare many approaches to find what works best for your task.",
    "ensure_quality": "Ensure quality over time, back testing prior bugs and benchmarking new approaches.",
    "evals_guide": "Evals Guide",
    "run_with_config": "Run this eval with the selected configuration?",
    "dont_close_page": "Don't close this page if you want to monitor progress.",
    "considerable_compute": "This may use considerable compute/credits.",
    "task_description": "Task Description:",
    "no_description": "No description provided.",
    "evaluation_steps": "Evaluation Steps:",
    "one_to_five_stars": "1 to 5 stars, where 5 is best",
    "pass_fail_desc": "0 is fail and 1 is pass",
    "pass_fail_critical_desc": "-1 is critical failure, 0 is fail, and 1 is pass",
    "main_page": {
      "title": "Eval: {name}",
      "subtitle": "Follow these steps to find the best way to evaluate and run your task",
      "sub_subtitle": "Read the Docs",
      "error_loading": "Error Loading Evaluator",
      "unknown_error": "An unknown error occurred",
      "evaluator_properties": "Evaluator Properties",
      "name": "Name",
      "description": "Description",
      "id": "ID",
      "eval_dataset": "Eval Dataset",
      "golden_dataset": "Golden Dataset",
      "golden_dataset_tooltip": "This is the dataset that we use to evaluate the quality of the evaluation method. Also called the 'Eval Method Dataset'. It needs to have human ratings.",
      "eval_algorithm": "Eval Algorithm",
      "eval_algorithm_tooltip": "The evaluation algorithm used by your selected eval method.",
      "eval_model": "Eval Model",
      "eval_model_tooltip": "The model used by your selected eval method.",
      "run_model": "Run Model",
      "run_model_tooltip": "The model used by your selected run method.",
      "run_prompt": "Run Prompt",
      "run_prompt_tooltip": "The prompt used by your selected run method.",
      "step_titles": {
        "define_goals": "Define Goals",
        "create_eval_data": "Create Eval Data",
        "human_ratings": "Human Ratings",
        "find_best_evaluator": "Find the Best Evaluator",
        "find_best_way": "Find the Best Way to Run this Task"
      },
      "step_tooltips": {
        "define_goals": "Each eval needs a set of quality goals to measure (aka 'eval scores'). You can add separate evals for different goals, or multiple goals to the same eval.",
        "create_eval_data": "Each eval needs two datasets: one for ensuring the eval works (eval set), and another to help find the best way of running your task (golden set). We'll help you create both with synthetic data!",
        "human_ratings": "A 'golden' dataset is a dataset of items that are rated by humans. Rating a 'golden' dataset lets us determine if the evaluator is working by checking how well it aligns to human preferences.",
        "find_best_evaluator": "Benchmark different evaluation methods (models, prompts, algorithms). We'll compare to your golden dataset to find the evaluator which best matches human preferences.",
        "find_best_way": "This tool will help your compare a variety of options for running this task and find the best one. You can compare different models, prompts, or fine-tunes."
      },
      "goals_description": "This eval has {count} goals: {goals}.",
      "data_status": {
        "empty": "Create data for this eval.",
        "sufficient": "You have {evalSize} eval items and {goldenSize} golden items.",
        "insufficient": "You require additional eval data. You only have {evalSize} eval items and {goldenSize} golden items. We suggest at least {minSize} items in each set.",
        "insufficient_eval": "You only have {evalSize} eval items. We suggest at least {minSize} items.",
        "insufficient_golden": "You only have {goldenSize} golden items. We suggest at least {minSize} items."
      },
      "add_eval_data": "Add Eval Data",
      "golden_dataset_status": {
        "empty": "Your golden dataset is empty. Add data to your golden dataset to get started.",
        "unrated": "In your golden dataset {issues}. Fully rate all items to to get the best results from your eval.",
        "complete": "All items in your golden dataset are fully rated.",
        "unrated_items": "{count} item{plural} {verb} unrated",
        "partially_rated_items": "{count} item{plural} {verb} partially unrated"
      },
      "rate_golden_dataset": "Rate Golden Dataset",
      "view_golden_dataset": "View Golden Dataset",
      "golden_dataset_filter_note": "Your golden dataset is filtered by {filter}. Please rate these entries in the dataset tab.",
      "eval_method_selected": "You've selected the eval method '{method}' using the model '{model}'.",
      "eval_method_not_selected": "Compare automated evals to find one that aligns with your human preferences.",
      "compare_eval_methods": "Compare Eval Methods",
      "run_method_selected": "You've selected the model '{model}' with the prompt '{prompt}'.",
      "run_method_not_selected": "Compare models, prompts and fine-tunes to find the most effective.",
      "compare_run_methods": "Compare Run Methods",
      "creating_eval_progress": {
        "title": "Creating Eval",
        "when_done_adding": "When you're done adding data, ",
        "when_done_rating": "When you're done rating, ",
        "when_done_comparing_eval": "When you're done comparing eval methods, ",
        "when_done_comparing_run": "When you're done comparing run methods, ",
        "return_to_eval": "return to the eval"
      },
      "edit_dialog": {
        "name": "Eval",
        "eval_name_label": "Eval Name",
        "eval_name_description": "A name to identify this eval.",
        "description_label": "Description",
        "description_description": "A description of the eval for you and your team."
      },
      "errors": {
        "unable_to_add_data": "Unable to add eval data. Please try again later.",
        "no_tag_found": "No eval or golden dataset tag found. If you're using a custom filter, please setup the dataset manually."
      }
    },
    "eval_configs": {
      "title": "Compare Evaluation Methods",
      "subtitle": "Find the evaluation method that best matches human-ratings",
      "sub_subtitle": "Read the docs",
      "error_loading": "Error Loading",
      "unknown_error": "An unknown error occurred",
      "evaluator_properties": "Evaluator Properties",
      "name": "Name",
      "description": "Description",
      "eval_method_dataset": "Eval Method Dataset",
      "correlation_title": "Correlation to Human Ratings",
      "correlation_subtitle": "Each score in this table is a measure for how much the eval method correlates to human ratings, using the selected scoring metric.",
      "score_error": "An unknown error occurred fetching scores.",
      "score_label": "Score",
      "score_types": {
        "kendalltau": "Kendall's Tau Correlation",
        "spearman": "Spearman Rank Correlation",
        "norm_mse": "Normalized Mean Squared Error",
        "mse": "Mean Squared Error",
        "norm_mae": "Normalized Mean Absolute Error",
        "mae": "Mean Absolute Error",
        "pearson": "Pearson Correlation"
      },
      "warnings": {
        "issues_to_resolve": "There are issues you should resolve before analyzing this data.",
        "zero_items": "There are zero items in your eval method dataset. Generate some runs in your dataset tab, and tag them to add them to your eval method dataset.",
        "not_rated": "{count} item(s) in your eval method dataset are not rated at all. Add human ratings to these items in the dataset tab.",
        "partially_rated": "{count} item(s) in your eval method dataset are only partially rated. Add human ratings for each score in the dataset tab.",
        "incomplete_evals": "You evals are incomplete. Click 'Run Evals' to generate scores for the missing items.",
        "select_winner": "Click 'Set as default' below to select a winner.",
        "small_dataset": "There are only {size} item(s) in your Eval Method Dataset. This is generally too small to get a sense of how eval methods perform."
      },
      "table": {
        "eval_method": "Eval Method",
        "eval_method_desc": "How task output is evaluated",
        "eval_instructions": "Eval Instructions",
        "method_label": "Method:",
        "provider_label": "Provider:",
        "name_label": "Name:",
        "progress_label": "Progress:",
        "default_badge": "Default",
        "set_as_default": "Set as default",
        "see_all": "See all",
        "none": "None",
        "na_tooltip": "There wasn't enough data, or variation in the data, to calculate a {correlation} correlation. Add more data to your eval method dataset, focusing on values which are missing (for example, if all current items pass, add some which fail).",
        "no_scores_tooltip": "No scores were found for this eval method. Click 'Run Eval' to generate scores and ensure your golden dataset has human ratings."
      },
      "score_tooltips": {
        "mae": "Mean absolute error. Lower is better. For {rating}.",
        "mse": "Mean squared error. Lower is better. For {rating}.",
        "norm_mse": "Normalized mean squared error. Lower is better. For {rating}.",
        "norm_mae": "Normalized mean absolute error. Lower is better. For {rating}.",
        "spearman": "Spearman's rank correlation. Higher is better. For {rating}.",
        "pearson": "Pearson's correlation. Higher is better. For {rating}.",
        "kendalltau": "Kendall's Tau correlation. Higher is better. For {rating}.",
        "five_star": "1 to 5 star rating",
        "pass_fail": "pass/fail rating",
        "pass_fail_critical": "pass/fail/critical rating"
      },
      "empty_state": {
        "title": "Create an Eval Method to Get Started",
        "description": "An evaluation method specifies how an eval is run (algorithm, model, instructions, etc).",
        "add_button": "Add Eval Method"
      },
      "instructions_dialog": {
        "title": "Instructions for Eval Method '{name}'"
      },
      "legend_dialog": {
        "title": "How to Compare Evaluation Methods",
        "description": "Each score is a correlation score between human ratings and the automated eval method's scores. Use these scores to find the eval method which best matches human ratings, and set it as your default eval method.",
        "quick_start_title": "Quick Start",
        "quick_start_1": "Add a variety of eval methods with different options (model, algorithm, instructions). Then click 'Run Eval' to generate scores from each eval method on your eval method dataset.",
        "quick_start_2": "We suggest you use Kendall's Tau correlation scores to compare results. Kendall's Tau scores range from -1.0 to 1. Higher values indicate higher correlation between the human ratings and the automated eval method's scores. The absolute value of Kendall's Tau scores will vary depending on how subjective your task is.",
        "quick_start_3": "Finally, set the eval method with the highest Kendall's Tau score as your default eval method.",
        "detailed_title": "Detailed Instructions",
        "detailed_description": "for more information, a detailed walkthrough, and technical details about each scoring metric."
      }
    }
  },
  "rating": {
    "pass": "Pass",
    "fail": "Fail",
    "critical": "Critical",
    "unrated": "Unrated",
    "custom_score": "custom score",
    "custom_type_not_supported": "Custom type not supported in UI"
  },
  "forms": {
    "required": "Required",
    "optional": "Optional",
    "field_required": "{label} is required",
    "field_too_long": "{label} must be no more than {maxLength} characters",
    "required_field": "This field is required",
    "invalid_email": "Please enter a valid email address",
    "invalid_url": "Please enter a valid URL",
    "invalid_number": "Please enter a valid number",
    "min_length": "Must be at least {min} characters",
    "max_length": "Must be no more than {max} characters",
    "password_mismatch": "Passwords do not match",
    "invalid_json": "Invalid JSON format",
    "file_too_large": "File is too large",
    "invalid_file_type": "Invalid file type",
    "unsaved_changes_warning": "You have unsaved changes which will be lost if you leave.\n\nPress Cancel to stay, OK to leave.",
    "please_correct_errors": "Please correct the errors above",
    "saved": "Saved",
    "form_list": {
      "remove_item_confirm": "Are you sure you want to remove {content_label} #{index}?",
      "remove": "Remove",
      "add": "Add {content_label}"
    }
  },
  "progress": {
    "in_progress": "In Progress"
  },
  "errors": {
    "eval_id_required": "Eval ID is required",
    "unknown_error": "An unknown error occurred"
  },
  "tooltips": {
    "info": "Information"
  },
  "dialog": {
    "confirm_delete": "Are you sure you want to delete this item?",
    "cannot_undo": "This action cannot be undone.",
    "delete_title": "Delete {name}?",
    "delete_confirm_message": "Are you sure you want to delete this {name}?",
    "delete_warning": "This cannot be undone. We suggest backing up your project before deleting.",
    "failed_to_delete": "Failed to delete.",
    "edit_title": "Edit {name}",
    "save_success": "Saved successfully",
    "save_error": "Failed to save"
  },
  "models": {
    "not_tested": "This model has not been tested with Kiln. It may not work as expected.",
    "not_recommended_data_gen": "This model is not recommended for use with data generation. It's known to generate incorrect data.",
    "no_logprobs": "This model does not support logprobs. It will likely fail when running a G-eval or other logprob queries.",
    "not_recommended_structured": "This model is not recommended for use with tasks requiring structured output. It fails to consistently return structured data.",
    "suggest_data_gen": "For data gen we suggest using a high quality model such as GPT 4.1, Sonnet, Gemini Pro or R1.",
    "suggest_evals": "For evals we suggest using a high quality model such as GPT 4.1, Sonnet, Gemini Pro or R1.",
    "model_label": "Model",
    "recommended": "Recommended",
    "untested_models": "Untested Models",
    "not_recommended": "Not Recommended",
    "not_recommended_data_gen_label": "Not Recommended - Data Gen Not Supported",
    "not_recommended_structured_label": "Not Recommended - Structured Output Fails",
    "not_recommended_logprobs_label": "Not Recommended - Logprobs Not Supported"
  },
  "tags": {
    "cannot_be_empty": "Tags cannot be empty",
    "no_spaces": "Tags cannot contain spaces. Use underscores.",
    "add_tag": "Add a tag"
  },
  "updates": {
    "check_for_update": "Check for Update",
    "current_version": "Current Version",
    "update_available": "Update Available",
    "version_available": "is available.",
    "download_update": "Download Update",
    "no_update_available": "No Update Available",
    "latest_version": "You are using the latest version of Kiln.",
    "error_checking": "Error Checking for Update"
  },
  "add_data": {
    "add_samples": "Add Samples to your Dataset",
    "add_for_eval": "Add Data for your Eval",
    "add_for_finetune": "Add Data for Fine-tuning",
    "follow_steps": "Follow these steps to manually tag existing data to be used for your",
    "adding_tags_proportions": "You will be adding tags in the following proportions:",
    "synthetic_data": "Synthetic Data",
    "synthetic_data_description": "Generate synthetic data using our interactive tool.",
    "upload_csv": "Upload CSV",
    "upload_csv_description": "Add data by uploading a CSV file.",
    "manually_run_task": "Manually Run Task",
    "manually_run_task_description": "Each run will be saved to your {reason_name}.",
    "manually_tag_existing_data": "Manually Tag Existing Data",
    "manually_tag_existing_data_description": "Tag existing data for use in your {reason_name}.",
    "data_added": "Data Added",
    "data_added_subtitle": "Your data has been added.",
    "return_to_eval": "Return to Eval",
    "return_to_finetune": "Return to Fine-Tune",
    "view_dataset": "View Dataset",
    "manually_tag_dialog_title": "Manually Tag Existing Data",
    "open_dataset": "Open Dataset",
    "dataset_page": "dataset page",
    "follow_steps_to_tag": "Follow these steps to manually tag existing data to be used for your {reason_name}.",
    "step_open_dataset": "Open the {dataset_link} in a new tab so you can follow these instructions.",
    "step_select_data": "Using the \"Select\" button, select the data you to tag. You can select many examples at once using the shift key.",
    "step_click_tag_single": "Click the \"Tag\" button, select \"Add Tag\", then add the tag \"{tag_name}\".",
    "step_click_tag_multiple": "Click the \"Tag\" button, select \"Add Tag\", then add the desired tag.",
    "step_repeat_for_tags": "Repeat steps 2-3 for each tag. Be sure to tag in the proportions described above.",
    "reason_names": {
      "dataset": "dataset",
      "eval": "eval",
      "fine_tune": "fine tune"
    }
  },
  "setup": {
    "welcome_to_kiln": "Welcome to Kiln",
    "easiest_way_to_build": "The easiest way to build AI products",
    "get_started": "Get Started",
    "view_our": "View our",
    "license_agreement": "License Agreement",
    "connect_ai_providers": "Connect AI Providers",
    "connect_providers_subtitle": "Kiln is free, but your need to connect API keys to use AI services.",
    "introduction": "Introduction",
    "continue": "Continue",
    "skip_tutorial": "Skip Tutorial",
    "newsletter": "Newsletter",
    "newsletter_subtitle": "Zero spam, unsubscribe any time, totally optional.",
    "newsletter_description": "Subscribe to our newsletter to learn about new features, updates, new models, and other Kiln AI news.",
    "subscribe": "Subscribe",
    "email": "Email",
    "subscribed": "Subscribed!",
    "subscribed_message": "Thanks for subscribing! ❤️",
    "continue_without_subscribing": "Continue Without Subscribing",
    "select_project_and_task": "Select a Project and Task",
    "select_project_and_task_subtitle": "Select a project and task to get started. You can also create a new projects and tasks."
  },
  "providers": {
    "add_models": "Add Models",
    "add_model": "Add Model",
    "add_new_api": "Add New API",
    "manage_providers": "Manage Providers & Models",
    "add_models_subtitle": "Each AI provider already includes models tested for Kiln. Add additional models here.",
    "saving": "Saving",
    "error_saving_model_list": "Error saving model list:",
    "add_model_from_provider": "Add a model from an existing provider.",
    "model_id_instructions": "Provide the exact model ID used by the provider API. For example, OpenAI's \"gpt-3.5-turbo\" or Groq's \"gemma2-9b-it\".",
    "model_provider": "Model Provider",
    "model_name": "Model Name",
    "invalid_model_error": "Invalid model provider or name. Please try again with all fields.",
    "no_response_error": "No response from server",
    "settings_not_found": "Settings not found",
    "connect_providers": {
      "connect": "Connect",
      "disconnect": "Disconnect",
      "connected": "Connected",
      "connecting": "Connecting",
      "manage": "Manage",
      "cancel": "Cancel",
      "error": "Error",
      "reload_page": "Reload page",
      "connect_title": "Connect {provider}",
      "cancel_setup": "Cancel setting up {provider}",
      "disconnect_confirm": "Are you sure you want to disconnect this provider? Your connection details will be deleted and can not be recovered.",
      "disconnect_failed": "Failed to disconnect provider. Unknown error.",
      "ollama_disconnect_message": "Ollama automatically connects to the localhost Ollama instance when it is running. It can't be manually disconnected. To change your preferred Ollama URL, turn of your localhost Ollama instance then return to this screen.",
      "set_custom_ollama_url": "Set Custom Ollama URL",
      "custom_ollama_url": "Custom Ollama URL",
      "custom_ollama_url_description": "By default, Kiln attempts to connect to Ollama running on localhost:11434. If you run Ollama on a custom URL or port, enter it here to connect.",
      "ollama_url_info": "It should included the http prefix, and the port number. For example, http://localhost:11434",
      "connect_custom_apis": "Connect Custom APIs",
      "custom_api_description": "Connect any any OpenAI compatible API by adding a base URL and API key.",
      "existing_apis": "Existing APIs",
      "add_new_api": "Add New API",
      "api_name": "API Name",
      "api_name_placeholder": "My home server",
      "api_name_info": "A name for this endpoint for you use. Example: 'My home server'",
      "base_url": "Base URL",
      "base_url_placeholder": "https://.../v1",
      "base_url_info": "The base URL of an OpenAI compatible API. For example, https://openrouter.ai/api/v1",
      "api_key": "API Key",
      "api_key_placeholder": "sk-...",
      "api_key_info": "The API key for the OpenAI compatible API.",
      "base_url_error": "Base URL must start with http",
      "remove": "Remove",
      "remove_provider_failed": "Failed to remove provider: {error}",
      "add": "Add",
      "ollama_connected": "Ollama connected.",
      "ollama_no_supported_models": "No supported models are installed -- we suggest installing some (e.g. 'ollama pull llama3.1').",
      "ollama_untested_models": "The following untested models are installed: {models}.",
      "ollama_supported_models": "The following supported models are available: {models}.",
      "ollama_custom_url": "Custom Ollama URL: {url}",
      "ollama_version_error": "Ollama version must be 0.5.0 or higher. Please update Ollama.",
      "ollama_no_models": "Ollama running, but no models available. Install some using ollama cli (e.g. 'ollama pull llama3.1').",
      "ollama_connection_failed": "Failed to connect. Ensure Ollama app is running.",
      "provider_descriptions": {
        "openrouter": "Proxies requests to OpenAI, Anthropic, and more. Works with almost any model.",
        "openai": "The OG home to GPT-4o and more. Supports fine-tuning.",
        "ollama": "Run models locally. No API key required.",
        "groq": "The fastest model host. Providing Llama, Gemma and Mistral models.",
        "fireworks_ai": "Open models (Llama, Phi), plus the ability to fine-tune.",
        "anthropic": "The home of Sonnet, Haiku, and Opus.",
        "gemini_api": "Google's Gemini API. Not to be confused with Vertex AI.",
        "azure_openai": "Microsoft's Azure OpenAI API.",
        "huggingface": "AI community hub, with many models.",
        "vertex": "Google's Vertex AI API. Not to be confused with Gemini AI Studio.",
        "together_ai": "Inference service from Together.ai",
        "amazon_bedrock": "So your company has an AWS contract?",
        "wandb": "Track and visualize your experiments.",
        "openai_compatible": "Connect any OpenAI compatible API."
      },
      "provider_warnings": {
        "openai": "Note: the OpenAI API requires a separate account from ChatGPT.",
        "azure_openai": "With Azure OpenAI, you must deploy each model manually.\nSee our docs for details: https://docs.getkiln.ai/docs/models-and-ai-providers#azure-openai-api",
        "vertex": "With Vertex AI, you must deploy some models manually.\nSee our docs for details: https://docs.getkiln.ai/docs/models-and-ai-providers#google-vertex-ai",
        "amazon_bedrock": "Bedrock is quite difficult to setup.\nFor beginners we suggest other providers, like OpenRouter, as they easier to setup and have more models."
      },
      "api_key_steps": {
        "openrouter": [
          "Go to https://openrouter.ai/settings/keys",
          "Create a new API Key",
          "Copy the new API Key, paste it below and click 'Connect'"
        ],
        "openai": [
          "Create an OpenAI Platform account at https://platform.openai.com/signup and add a payment method.",
          "Go to https://platform.openai.com/account/api-keys",
          "Click 'Create new secret key'",
          "Copy the new secret key, paste it below and click 'Connect'"
        ],
        "groq": [
          "Go to https://console.groq.com/keys",
          "Create an API Key",
          "Copy the new key, paste it below and click 'Connect'"
        ],
        "fireworks_ai": [
          "Go to https://fireworks.ai/account/api-keys",
          "Create a new API Key and paste it below",
          "Go to https://fireworks.ai/account/profile",
          "Copy the Account ID, paste it below, and click 'Connect'"
        ],
        "anthropic": [
          "Go to https://console.anthropic.com/settings/keys",
          "Create a new API Key",
          "Copy the new API Key, paste it below and click 'Connect'"
        ],
        "gemini_api": [
          "Go to https://aistudio.google.com/app/apikey",
          "Create a new API Key",
          "Copy the new API Key, paste it below and click 'Connect'"
        ],
        "azure_openai": [
          "Open the Azure portal, and navigate to the Azure OpenAI resource you want to use. Create a new resource if you don't have one, we suggest 'East US2' for maximal model support.",
          "Open the Keys & Endpoint section. Find your API Key and Endpoint URL. The Endpoint URL will look like https://<your-resource-name>.openai.azure.com",
          "Copy the API Key and Endpoint URL, paste them below and click 'Connect'"
        ],
        "huggingface": [
          "Go to https://huggingface.co/settings/tokens",
          "Create a new Access Token",
          "Copy the new Access Token, paste it below and click 'Connect'"
        ],
        "vertex": [
          "Create a Google Cloud account.",
          "Install the glcoud CLI, then run `gcloud auth application-default login` in the terminal. This will add Google Vertex credentials to you environment.",
          "Create a project in the console, enable Vertex AI for that project, and click 'Enable Recommended APIs' in the Vertex AI console.",
          "Add the project ID below. Be sure to use the project ID, not the project name.",
          "Add a Google Cloud location, example: 'us-central1'. We suggest 'us-central1' as it has the widest model support.",
          "Click connect."
        ],
        "together_ai": [
          "Create a Together account.",
          "Create an API Key (or user key) here: https://api.together.ai/settings/api-keys",
          "Copy the API Key, paste it below and click 'Connect'"
        ],
        "amazon_bedrock": [
          "Go to https://us-west-2.console.aws.amazon.com/bedrock/home?region=us-west-2#/overview - be sure to select us-west-2, as it has the most models, and Kiln defaults to this region",
          "Request model access for supported models like Llama and Mistral",
          "Create an IAM Key using this guide https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html and be sure to select 'AmazonBedrockFullAccess' policy when creating the IAM user",
          "Get the access key ID and secret access key for the new user. Paste them below and click 'Connect'"
        ],
        "wandb": [
          "Create a Weights & Biases account at https://wandb.ai, or host your own instance.",
          "If you host your own instance, set the base URL below. Then create an API key and entering it below.",
          "If you use the hosted version, go to https://wandb.ai/settings#api, get your API key, and paste it below.",
          "Click 'Connect'"
        ]
      },
      "api_key_fields": {
        "api_key": "API Key",
        "account_id": "Account ID",
        "endpoint_url": "Endpoint URL",
        "project_id": "Project ID",
        "project_location": "Project Location",
        "access_key": "Access Key",
        "secret_key": "Secret Key",
        "base_url": "Base URL"
      },
      "pills": {
        "tuneable": "Tuneable"
      }
    }
  },
  "data_generation": {
    "add_top_level_topics": "Add Top Level Topics",
    "add_top_level_data": "Add Top Level Data",
    "add_data_to_all": "Add Data to All",
    "add_subtopics": "Add subtopics",
    "add_data": "Add data",
    "add_data_to_all_subtopics": "Add data to all",
    "add_custom_topics": "Add Custom Topics",
    "add_guidance": "Add Guidance",
    "edit_guidance": "Edit Guidance",
    "human_guidance": "Human Guidance",
    "guidance_description": "Add human guidance to improve or steer AI-generated data. Learn more and see examples",
    "guidance_label": "Guidance to help the model generate relevant data:",
    "clear": "Clear",
    "done": "Done",
    "run_generation": "Run generation and add all items to your dataset.",
    "in_the_docs": "in the docs",
    "collapse": "Collapse",
    "expand": "Expand",
    "generate_topics": "Generate topics",
    "topic_count": "Topic Count",
    "generate_n_topics": "Generate {count} Topics",
    "custom_topics": "Custom topics",
    "comma_separated_list": "Comma separated list",
    "add_list_subtopics": "Add a list of subtopics",
    "to_path": "to {path}",
    "intro": {
      "synthetic_data_tips": "Synthetic Data Tips",
      "tip_1": "Adding topics will help generate diverse data. They can be nested, forming a topic tree.",
      "tip_2": "Adding human guidance can shape and improve the AI-generated data.",
      "guide": "Guide",
      "read_the_docs": "Read the Docs",
      "generate_title": "Generate Synthetic Data for Training or Evaluations",
      "generate_description": "We suggest adding synthetic data as part of creating an eval or creating a fine-tuning training set.",
      "create_eval": "Create an Eval",
      "create_finetune": "Create a Fine-Tune",
      "proceed_to_generator": "Proceed to Generator"
    }
  },
  "prompts": {
    "page_title": "Prompts",
    "page_subtitle": "Prompts for the task \"{task_name}\"",
    "page_sub_subtitle": "Read the Docs",
    "generators_title": "Prompt Generators",
    "generators_description": "Generators build prompts dynamically based on the {task_link} and the {dataset_link}. For example, the multi-shot prompt appends highly rated dataset samples to the prompt.",
    "task_default_prompt_link": "task's default prompt",
    "task_dataset_link": "task's dataset",
    "no_generators_found": "No prompt generators found for this task.",
    "saved_prompts_title": "Saved Prompts",
    "create_new_prompt_link": "Create a new prompt",
    "no_saved_prompts_found": "No saved prompts found for this task. {create_link}.",
    "create_one_now": "Create one now",
    "table_headers": {
      "name": "Name",
      "description": "Description",
      "name_and_description": "Name & Description",
      "type": "Type",
      "prompt_preview": "Prompt Preview"
    },
    "prompt_types": {
      "custom": "Custom",
      "fine_tune_prompt": "Fine Tuning Prompt",
      "eval_prompt": "Eval Prompt",
      "unknown": "Unknown"
    },
    "task_link_error": "This link is to another task's prompts. Either select that task in the sidebar, or click prompts in the sidebar to load the current task's prompts.",
    "create_prompt": "Create a Prompt",
    "create_prompt_subtitle": "For the task \"{task_name}\"",
    "prompt_name": "Prompt Name",
    "prompt_name_description": "A short name to uniquely identify this prompt.",
    "prompt_description": "Prompt Description",
    "prompt_description_description": "A description of the prompt for your reference.",
    "prompt_label": "Prompt",
    "prompt_input_description": "A prompt to use for this task.",
    "prompt_info_description": "A LLM prompt such as 'You are a helpful assistant.'. This prompt is specific to this task. To use this prompt after creation, select it from the prompts dropdown.",
    "chain_of_thought": "Chain of Thought",
    "chain_of_thought_description": "Should this prompt use chain of thought?",
    "chain_of_thought_disabled": "Disabled",
    "chain_of_thought_enabled": "Enabled",
    "chain_of_thought_instructions": "Chain of Thought Instructions",
    "chain_of_thought_instructions_description": "Instructions for the model's 'thinking' prior to answering. Required for chain of thought prompting.",
    "chain_of_thought_default": "Think step by step, explaining your reasoning.",
    "create_prompt_button": "Create Prompt",
    "generator_details": {
      "title": "Prompt Generator",
      "generator_description": "Generator Description",
      "how_to_improve": "How to Improve this Prompt",
      "improve_description": "To improve the quality of this prompt,",
      "edit_task_link_text": "edit the task's prompt/instructions",
      "requirements_text": " or requirements",
      "additional_improvement": ", or add more high quality samples to your dataset by",
      "run_link_text": "running the task",
      "additional_improvement_suffix": " and rating/repairing the output.",
      "generated_prompt": "Generated Prompt",
      "generated_prompt_description": "This is the current prompt generated by the \"{generator_name}\" generator. It may change over time if your underlying dataset or task definition changes.",
      "task_link_error": "This link is to another task's prompts. Either select that task in the sidebar, or click prompts in the sidebar to load the current task's prompts."
    },
    "saved_prompt": {
      "title": "Saved Prompt",
      "details": "Details",
      "prompt_not_found": "Prompt not found.",
      "task_link_error": "This link is to another task's prompt. Either select that task in the sidebar, or click 'Prompts' in the sidebar to load the current task's prompts.",
      "edit_note": "Note: Prompt content can't be edited to ensure consistency with prior runs. Instead, copy this prompt and create a new copy.",
      "details_fields": {
        "id": "ID",
        "name": "Name",
        "description": "Description",
        "created_by": "Created By",
        "created_at": "Created At",
        "chain_of_thought": "Chain of Thought",
        "source_generator": "Source Generator"
      }
    }
  },
  "finetune": {
    "empty_finetune": {
      "title": "Fine-Tuning Learns from Your Dataset to Create Custom Models",
      "description": "Fine-tuned models can be faster, cheaper and more accurate than standard models.",
      "create_button": "Create a Fine-Tune",
      "guide_button": "Fine Tuning Guide"
    },
    "select_dataset": "Select Fine-Tuning Dataset",
    "reuse_existing_dataset": "Reuse Dataset from an Existing Fine-Tune",
    "reuse_existing_description": "When comparing multiple base models, it's best to use exactly the same fine-tuning dataset.",
    "create_new_dataset": "Create a New Fine-Tuning Dataset",
    "create_new_description": "Create a new fine-tuning dataset by selecting a subset of your data.",
    "add_finetune_data": "Add Fine-Tuning Data",
    "add_data_description": "Add data for fine-tuning using synthetic data generation, CSV upload, or by tagging existing data.",
    "add_additional_data": "add additional fine-tuning data",
    "new_finetune_dataset": "New Fine-Tuning Dataset",
    "snapshot_subset": "Snapshot a subset of your dataset to be used for fine-tuning.",
    "dataset_filter_tag": "Dataset Filter Tag (Required)",
    "select_tag_description": "Select a tag. Only samples with this tag will be used in fine-tuning.",
    "available_tags_info": "Available tags start with 'fine_tune'. You can create custom tags with this prefix to organize different fine-tuning datasets.",
    "filter_reasoning_samples": "Filter to Reasoning Samples",
    "reasoning_samples_info": "Only samples with a thinking data (reasoning or chain of thought) will be included in the training dataset. Required when training a reasoning model.",
    "filter_highly_rated": "Filter to Highly Rated Samples",
    "highly_rated_info": "Only samples with an overall rating of 4 or 5 stars will be included in the training dataset. Required when training a high-quality model.",
    "advanced_options": "Advanced Options",
    "dataset_splits": "Dataset Splits",
    "splits_description": "Select ratios for splitting the data into training, validation, and test.",
    "splits_info": "If in doubt, leave the the recommended value. If you're using an external test set such as Kiln Evals, you don't need a test set here.",
    "train_val_80_20": "80% Training, 20% Validation (Recommended)",
    "train_test_80_10_10": "80% Training, 10% Test, 10% Validation",
    "train_test_val_60_20_20": "60% Training, 20% Test, 20% Validation",
    "train_test_val_80_10_10": "80% Training, 10% Test, 10% Validation",
    "all_training": "100% Training",
    "create_dataset": "Create Dataset",
    "change_dataset": "Change Dataset",
    "training_dataset_details": "Training Dataset Details",
    "dataset_has_splits": "The selected dataset has {count} {count, plural, one {split} other {splits}}:",
    "split_train": "Will be used for training",
    "split_val": "May be used for validation during fine-tuning",
    "split_test": "Will not be used, reserved for later evaluation",
    "split_all": "Will be used for training",
    "examples_count": "{count} examples",
    "dataset_created": "Dataset '{name}' created {date}",
    "select_existing_dataset": "Select Dataset from an Existing Fine-Tune",
    "select_existing_description": "Select an existing fine-tuning dataset to use exactly the same data for this fine-tune.",
    "no_existing_datasets": "No existing fine-tune datasets found.",
    "dataset_name": "Dataset Name",
    "dataset_size": "Dataset Size",
    "tunes_using_dataset": "Tunes Using Dataset",
    "in_split": "in '{split}'",
    "zero_samples_error": "Zero samples match your filters. Your dataset must include at least 1 sample. Please try a different filter or add more data.",
    "few_samples_warning": "The dataset will only have {count} samples. We suggest at least 50 samples for fine-tuning.",
    "samples_count_info": "The dataset will have {count} samples.",
    "creating_finetune": "Creating Fine-Tune",
    "when_done_adding": "When you're done adding data, ",
    "return_to_finetuning": "return to fine-tuning",
    "invalid_splits_parameter": "Invalid splits parameter, using default",
    "samples_assigned_tags": "Samples will be assigned the following tags: {tags}",
    "create_new_finetune": "Create a New Fine Tune",
    "finetune_subtitle": "Fine-tuned models learn from your dataset.",
    "step1_title": "Step 1: Select Base Model to Fine-Tune",
    "step2_title": "Step 2: Select Fine-Tuning Dataset",
    "step3_title": "Step 3: Options",
    "step4_title": "Step 4: Download JSONL",
    "model_provider_label": "Model & Provider",
    "model_provider_description": "Select which model to fine-tune. Alternatively, download a JSONL file to fine-tune using any infrastructure.",
    "model_provider_info": "Connect providers in settings for 1-click fine-tuning. Alternatively, download a JSONL file to fine-tune using any infrastructure, like Unsloth or Axolotl.",
    "select_model_to_finetune": "Select a model to fine-tune",
    "requires_api_key": " --- Requires API Key in Settings",
    "connect_providers_warning": "For 1-click fine-tuning connect OpenAI, Fireworks, Together, or Google Vertex.",
    "select_dataset_description": "Select a dataset to use for this fine-tune.",
    "dataset_info_tooltip": "A fine-tuning dataset is a subset of your dataset which is used to train and validate the fine-tuned model. This is typically a subset of your dataset, which is intentionally kept separate from your eval data.",
    "system_prompt_description": "The system message to use for fine-tuning. Choose the prompt you want to use with your fine-tuned model.",
    "system_prompt_info": "There are tradeoffs to consider when choosing a system prompt for fine-tuning. Read more: https://platform.openai.com/docs/guides/fine-tuning/#crafting-prompts",
    "custom_finetune_prompt": "Custom Fine Tuning Prompt",
    "custom_system_prompt_label": "Custom System Prompt",
    "custom_system_prompt_description": "Enter a custom system prompt to use during fine-tuning.",
    "custom_thinking_instructions_label": "Custom Thinking Instructions",
    "custom_thinking_instructions_description": "Instructions for the model's 'thinking' stage, before returning the final response.",
    "custom_thinking_instructions_info": "When training with intermediate results (reasoning, chain of thought, etc.), this prompt will be used to ask the model to 'think' before returning the final response.",
    "reasoning_label": "Reasoning",
    "reasoning_description": "Should the model be trained on reasoning/thinking content?",
    "reasoning_info": "If you select 'Thinking', the model training will include thinking such as reasoning or chain of thought. Use this if you want to call the tuned model with a chain-of-thought prompt for additional inference time compute.",
    "disabled_recommended": "Disabled - (Recommended)",
    "thinking_learn_both": "Thinking - Learn both thinking and final response",
    "thinking_r1_compatible": "Thinking (R1 compatible) - Learn both thinking and final response",
    "thinking_dataset_warning": "You are training a model for inference-time thinking, but are not using a dataset filtered to samples with reasoning or chain-of-thought training data. This is not recommended, as it may lead to poor performance. We suggest creating a new dataset with a thinking filter.",
    "thinking_r1_warning": "You are training a 'thinking' model, but did not explicitly select a dataset filtered to samples with reasoning or chain-of-thought training data. If any of your training samples are missing reasoning data, it will error. If your data contains reasoning, you can ignore this warning.",
    "finetune_name_label": "Name",
    "finetune_name_description": "A name to identify this fine-tune. Leave blank and we'll generate one for you.",
    "finetune_description_label": "Description",
    "finetune_description_description": "An optional description of this fine-tune.",
    "hyperparameter_info": "If you aren't sure, leave blank for default/recommended value. Ensure your value is valid for the type (e.g. an integer can't have decimals).",
    "start_finetune_job": "Start Fine-Tune Job",
    "download_jsonl_description": "Download JSONL files to fine-tune using any infrastructure, such as {unsloth} or {axolotl}.",
    "download_split": "Download Split: {split} ({count} examples)",
    "error_loading_models": "Error Loading Available Models and Datasets",
    "finetune_created_title": "Fine Tune Created",
    "finetune_created_subtitle": "It will take a while to complete training.",
    "view_finetune_job": "View Fine Tune Job",
    "invalid_model_provider": "Invalid model or provider",
    "invalid_integer": "Invalid integer value for {name}: {value}",
    "invalid_float": "Invalid float value for {name}: {value}",
    "invalid_boolean": "Invalid boolean value: {value}",
    "invalid_hyperparameter_type": "Invalid hyperparameter type: {type}",
    "could_not_create_dataset_split": "Could not create a dataset split for fine-tuning.",
    "could_not_load_hyperparameters": "Could not load hyperparameters for fine-tuning.",
    "invalid_response_from_server": "Invalid response from server",
    "type_integer": "Integer",
    "type_float": "Float",
    "type_boolean": "Boolean - 'true' or 'false'",
    "type_string": "String",
    "download_formats": {
      "openai_chat_jsonl": "Download: OpenAI chat format (JSONL)",
      "openai_chat_json_schema_jsonl": "Download: OpenAI chat format with JSON response (JSONL)",
      "openai_chat_toolcall_jsonl": "Download: OpenAI chat format with tool call response (JSONL)",
      "huggingface_chat_template_jsonl": "Download: HuggingFace chat template (JSONL)",
      "huggingface_chat_template_toolcall_jsonl": "Download: HuggingFace chat template with tool calls (JSONL)",
      "vertex_gemini": "Download: Google Vertex-AI Gemini format (JSONL)"
    },
    "details": {
      "title": "Fine Tune",
      "subtitle": "Fine-tune models for the current task.",
      "read_docs": "Read the Docs",
      "create_fine_tune": "Create Fine Tune",
      "table_headers": {
        "name": "Name",
        "type": "Type",
        "provider": "Provider",
        "base_model": "Base Model",
        "status": "Status",
        "created_at": "Created At"
      },
      "status_values": {
        "pending": "Pending",
        "running": "Running",
        "completed": "Completed",
        "failed": "Failed",
        "unknown": "Unknown"
      },
      "error_loading_finetunes": "Error Loading Fine Tunes",
      "could_not_load_finetunes": "Could not load finetunes. This task may belong to a project you don't have access to.",
      "unknown_error_occurred": "An unknown error occurred",
      "details_section": "Details",
      "status_section": "Status",
      "training_prompt_section": "Training Prompt",
      "system_prompt_label": "System Prompt",
      "thinking_instructions_label": "Thinking Instructions",
      "kiln_id": "Kiln ID",
      "name": "Name",
      "description": "Description",
      "provider": "Provider",
      "base_model": "Base Model",
      "model_id": "Model ID",
      "job_id": "Job ID",
      "created_at": "Created At",
      "created_by": "Created By",
      "type": "Type",
      "type_info": "The type of model, determined by the strategy used to build the training data for the fine tune. Standard will only learn from the final output of the task run. Reasoning also trains on intermediate outputs (reasoning or chain of thought). You should typically call a fine-tune with the same strategy it was trained with.",
      "status": "Status",
      "status_message": "Status Message",
      "job_dashboard": "Job Dashboard",
      "dashboard": "Dashboard",
      "reload_status": "Reload Status",
      "not_completed": "Not completed",
      "unknown": "Unknown",
      "error_loading": "Error Loading Available Models and Datasets",
      "unknown_error": "An unknown error occurred"
    }
  },
  "tutorial": {
    "data_driven_improvements": "Data-driven improvements",
    "data_driven_promo_1": "Rate model outputs to understand what's working",
    "data_driven_promo_2": "Run evals to measure performance across your dataset",
    "data_driven_promo_3": "Track improvements over time with version control",
    "collaborate_team": "Collaborate with your team",
    "collaborate_promo_1": "Share projects and datasets with team members",
    "collaborate_promo_2": "Review and rate outputs together",
    "collaborate_promo_3": "Build better AI products as a team",
    "find_best_way": "Find the best way to solve your problem",
    "find_best_promo_1": "Compare different models and providers",
    "find_best_promo_2": "Test various prompting strategies",
    "find_best_promo_3": "Optimize for cost, speed, and quality",
    "finetune_synthetic": "Fine-tune with synthetic data",
    "finetune_promo_1": "Generate high-quality training data automatically",
    "finetune_promo_2": "Fine-tune models for your specific use case",
    "finetune_promo_3": "Improve performance beyond what prompting can achieve",
    "library_api": "Library and API",
    "library_promo_1": "Integrate Kiln into your existing workflow",
    "library_promo_2": "Use our Python library or REST API"
  },
  "test": {
    "project_name": "Test Project",
    "project_description": "Test Description",
    "user_name": "Test User",
    "provider_name": "Test Provider",
    "unknown": "Unknown"
  },
  "json_schema": {
    "property_number": "Property #{number}",
    "remove": "remove",
    "property_name": "Property Name",
    "type": "Type",
    "required": "Required",
    "description": "Description",
    "add_property": "Add Property",
    "raw_json_schema": "Raw JSON Schema",
    "raw_json_schema_info": "See json-schema.org for more information on the JSON Schema spec.",
    "revert_to_visual_editor": "Revert to Visual Editor",
    "not_supported_by_visual_editor": "Not Supported by the Visual Editor",
    "switch_to_raw_json_schema": "Switch to Raw JSON Schema",
    "switch_to_raw_json_schema_question": "Switch to Raw JSON Schema?",
    "raw_json_schema_description": "Raw JSON Schema will give you more control over the structure of your data, including arrays, nested objects, enums and more.",
    "advanced_users_only": "Advanced Users Only",
    "advanced_users_warning": "Raw JSON Schema provides advanced functionality, but requires technical expertise. Invalid schemas will cause task failures.",
    "confirm_remove_property": "Are you sure you want to remove Property #{number}?\n\nIt has content which hasn't been saved.",
    "confirm_revert_to_visual": "Revert to the visual schema editor?\n\nChanges made to the raw JSON schema will be lost.",
    "types": {
      "string": "String",
      "number": "Number",
      "integer": "Integer",
      "boolean": "Boolean",
      "array": "Array",
      "object": "Object",
      "enum": "Enum",
      "other": "More..."
    },
    "required_options": {
      "true": "True",
      "false": "False"
    },
    "errors": {
      "property_empty": "Property is empty. Please provide a name.",
      "property_special_chars": "Property name only contains special characters. Must be alphanumeric. Provided name with issues: {name}",
      "property_not_allowed": "Property not allowed in JSON schema: {property}",
      "empty_string_non_string": "Empty string provided for non-string property: {property}",
      "boolean_invalid": "Boolean property must be 'true' or 'false': {property}",
      "integer_invalid": "Property {property} must be an integer, got: {value}",
      "array_invalid": "Property {property} must be an array, got: {value}",
      "array_json_invalid": "Property {property} must be a valid JSON array, got: {value}",
      "object_invalid": "Property {property} must be a valid JSON object, got: {value}",
      "object_json_invalid": "Property {property} must be a valid JSON object, got: {value}",
      "unsupported_type": "Unsupported property type: {type} for property {property}. This may be supported by the python framework, but is not yet supported in the UI.",
      "required_property_missing": "Required property not provided: {property}",
      "schema_validation_failed": "The data did not match the required JSON schema.",
      "example_property_title": "Example Property",
      "example_property_description": "Replace this with your own property"
    }
  },
  "formatters": {
    "unknown": "Unknown",
    "just_now": "just now",
    "one_minute_ago": "1 minute ago",
    "minutes_ago": "minutes ago",
    "today": "today",
    "eval_config": {
      "g_eval": "G-Eval",
      "llm_as_judge": "LLM as Judge"
    },
    "data_strategy": {
      "standard": "Standard",
      "reasoning": "Reasoning"
    },
    "rating": {
      "five_star": "5 star",
      "pass_fail": "Pass/Fail",
      "pass_fail_critical": "Pass/Fail/Critical"
    }
  },
  "ui": {
    "fancy_select": {
      "select_option": "Select an option"
    },
    "warning": {
      "exclamation_icon_alt": "Warning icon",
      "info_icon_alt": "Information icon",
      "check_icon_alt": "Success icon"
    }
  },
  "stores": {
    "fine_tune_prompt": "Fine-Tune Prompt",
    "model_id_prefix": "Model ID:"
  }
}
